{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9c2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--image_size', type=int, default=256, help='Image size of training images')\n",
    "parser.add_argument('--style_image_size', type=int, default=256, help='Image size of the style (if equal to 0, will not resize). Resizing leads to very different results, I recommended using 256, 512 or 0 and see which one you prefer.')\n",
    "parser.add_argument('--batch_size', type=int, default=8, help='Batch size, paper use 4.')\n",
    "parser.add_argument('--n_colors', type=int, default=3)\n",
    "parser.add_argument('--H_size', type=int, default=16, help='Number of filters in the image transformator. Paper use 32 but official implementation recommend only using 16 for faster speed while retaining quality.')\n",
    "parser.add_argument('--Residual_blocks', type=int, default=5, help='Number of residual blocks')\n",
    "parser.add_argument('--SELU', type=bool, default=False, help='Using scaled exponential linear units (SELU) which are self-normalizing instead of ReLU with BatchNorm. Do not use.')\n",
    "parser.add_argument('--norm_type', default='instance', help='If \"instance\" uses instance normalization; if \"batch\" uses batch normalization.')\n",
    "parser.add_argument('--padding', default='reflect', help='If \"reflect\" uses reflection padding; if \"zero\" uses zero padding.')\n",
    "parser.add_argument('--lr', type=float, default=.001, help='Learning rate')\n",
    "parser.add_argument('--n_epoch', type=int, default=2, help='Number of epochs. 2 is generally enough, but more can be better, just Ctrl-C to stop it when you feel like it is good enough.')\n",
    "parser.add_argument('--beta1', type=float, default=0.9, help='Adam betas[0]')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='Adam betas[1]')\n",
    "parser.add_argument(\"--content_weight\", type=float, default=1, help=\"Weight of content loss\")\n",
    "parser.add_argument(\"--style_weight\", type=float, default=5, help=\"Weight of style loss. Make bigger or smaller depending on observed results vs desired results.\")\n",
    "parser.add_argument(\"--total_variation_weight\", type=float, default=1e-6, help=\"Weight of total variation loss (Should be between 1e-4 and 1e-6)\")\n",
    "parser.add_argument(\"--feature\", type=int, default=1, help=\"Contant loss feature used: 0=relu1_2, 1=relu2_2, 2=relu3_3, 3=relu4_3. Paper use relu2_2, official implementation use relu3_3.\")\n",
    "parser.add_argument(\"--NN_conv\", type=bool, default=False, help=\"This is highly recommended. This approach minimize checkerboard artifacts during training. Uses nearest-neighbor resized convolutions instead of strided convolutions (https://distill.pub/2016/deconv-checkerboard/ and github.com/abhiskk/fast-neural-style).\")\n",
    "parser.add_argument(\"--ANTIALIAS\", type=bool, default=False, help=\"Use antialiasing instead of bilinear to resize images. Sightly slower but sightly better quality.\")\n",
    "parser.add_argument('--seed', type=int)\n",
    "parser.add_argument('--input_folder', default='/mnt/sdb2/Datasets/COCO', help='input folder (Coco dataset for training, whichever dataset after to apply style)')\n",
    "parser.add_argument('--style_picture', default='/mnt/sdb2/styles/candy.jpg', help='Style picture')\n",
    "parser.add_argument('--VGG16_folder', default='/mnt/sdb2/VGG16', help='folder for the VGG16 (Will be downloaded automatically into this folder)')\n",
    "parser.add_argument('--output_folder', default='/mnt/sdb2/Output/FNT', help='output folder')\n",
    "parser.add_argument('--model_load', default='', help='Full path to transformation model to load (ex: /home/output_folder/run-5/models/G_epoch_11.pth)')\n",
    "parser.add_argument('--trained_model', type=bool, default=False, help='If True, the model has been trained and we only want it to generate the pictures (not resized).')\n",
    "parser.add_argument('--cuda', type=bool, default=True, help='enables cuda')\n",
    "parser.add_argument('--n_gpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--n_workers', type=int, default=2, help='Number of subprocess to use to load the data. Use at least 2 or the number of cpu cores - 1.')\n",
    "\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed423349",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_run = wandb.init(project=\"cat-generation-FastNeuralTransfer\",config={\"architecture\": \"FastNeuralTransfer\"})\n",
    "wandb.config.update(args) # adds all of the arguments as config variables\n",
    "param = wandb.config\n",
    "\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15242702",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1043885275.py, line 113)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [2]\u001b[0;36m\u001b[0m\n\u001b[0;31m    if param.norm_type = 'batch':\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "import os\n",
    "run=0\n",
    "base_dir = f\"{param.output_folder}/run-{run}/\"\n",
    "while os.path.exists(base_dir):\n",
    "    run+=1\n",
    "    base_dir = f\"{param.output_folder}/run-{run}\"\n",
    "os.makedirs(base_dir)\n",
    "os.makedirs(f\"{base_dir}/models\")\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transf\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "# from PIL import Image\n",
    "import PIL\n",
    "import imageio as iio\n",
    "\n",
    "from torch.utils.serialization import load_lua\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if param.cuda:\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "from IPython.display import Image\n",
    "to_img = transf.ToPILImage()\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "param.seed = param.seed or random.randint(1,10000)\n",
    "print(f\"Random Seed: {param.seed}\")\n",
    "wandb.log({\"Random\": param.seed})\n",
    "random.seed(param.seed)\n",
    "torch.manual_seed(param.seed)\n",
    "\n",
    "if param.cuda:\n",
    "    torch.cuda.manual_seed_all(param.seed)\n",
    "    \n",
    "## Transforming images\n",
    "trans = []\n",
    "if not param.trained_model:\n",
    "    if param.ANTIALIAS:\n",
    "        trans.append(transf.Scale(param.image_size,PIL.Image.ANTIALIAS))\n",
    "    else:\n",
    "        trans.append(transf.Scale(param.image_size,PIL.Image.BILINEAR))\n",
    "    trans.append(transfCenterCrop(param.image_size))\n",
    "trans.append(transf.ToTensor())\n",
    "trans.append(transf.ToTensor())\n",
    "trans.append(transf.Lambda(lambda x: x.mul(255)))\n",
    "trans = transf.Compose(trans)\n",
    "\n",
    "trans_style = []\n",
    "if param.style_image_size>0:\n",
    "    if param.ANTIALIAS:\n",
    "        trans_style.append(transf.Scale(param.style_image_size,PIL.Image.ANTIALIAS))\n",
    "    else:\n",
    "        trans_style.append(transf.Scale(param.style_image_size,PIL.Image.BILINEAR))\n",
    "    trans_style.append(transf.CenterCrop(param.style_image_size))\n",
    "trans_style.append(transf.ToTensor())\n",
    "trans_style.append(transf.Lambda(lambda x: x.mul(255)))\n",
    "trans_style = transf.Compose(trans_style)\n",
    "if param.trained_model:\n",
    "    param.batch_size=1\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.total_imgs = natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = PIL.Image.open(img_loc).convert(\"RGB\")\n",
    "        # image = iio.imread(img_loc)\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image\n",
    "    \n",
    "my_dataset = CustomDataSet(param.input_folder, transform=trans)\n",
    "dataset = torch.utils.data.DataLoader(my_dataset , batch_size=param.batch_size, shuffle=True, \n",
    "                               num_workers=param.n_workers)\n",
    "\n",
    "style_picture = PIL.Image.open(param.style_picture)\n",
    "style_picture = trans_style(style_picture)\n",
    "\n",
    "## Models\n",
    "# Reflection padding is an alternative to 0 padding (like looking at water reflection)\n",
    "# Official implementation of the paper use only reflection paddding in the downsample block but I use it everywhere, I'm not sure if it makes a difference.\n",
    "\n",
    "# Set batch norm or instance norm\n",
    "import functools\n",
    "if param.norm_type = 'batch':\n",
    "    Norm2D = functools.partial(nn.BatchNormed)\n",
    "elif param.norm_type = 'instance':\n",
    "    Norm2D = functools.partial(nn.InstanceNorm2d)\n",
    "    \n",
    "# Padding\n",
    "if param.padding == \"reflect\":\n",
    "    pad=0\n",
    "if param.padding == \"zero\":\n",
    "    pad = 1\n",
    "    \n",
    "# Residual Block of Generator\n",
    "class Residual_block(torch.nn.Module):\n",
    "    def __init__(self,h_size):\n",
    "        super(Residual_block,self).__init__()\n",
    "        # Two conv layer with same output size\n",
    "        model=[]\n",
    "        if param.padding ==\"reflect\":\n",
    "            model+=[nn.ReflectionPad2d(padding=1)]\n",
    "        model+=[nn.Conv2d(h_size,h_size,kernel_size=3,stride=1,padding=pad)]\n",
    "        if param.SELU:\n",
    "            model+=[torch.nn.SELU()]\n",
    "        else:\n",
    "            model+=[Norm2D(h_size),nn.ReLU(True)]\n",
    "        if param.padding ==\"reflect\":\n",
    "            model+=[nn.ReflectionPad2d(padding=1)]\n",
    "        model += [nn.Conv2d(h_size,h_size,kernel_size=3,stride=1,padding=pad)]\n",
    "        if not param.SELU:\n",
    "            model+=[Norm2D(h_size)]\n",
    "        self.model=nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        # Return itself + the result of the two convolutions\n",
    "        output = self.model(input)+input\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd9746c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
